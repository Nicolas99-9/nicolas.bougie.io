<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation</title>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;500;600;700&family=Source+Code+Pro:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #1a73e8;
            --secondary-color: #5f6368;
            --accent-color: #4285f4;
            --bg-color: #ffffff;
            --text-color: #202124;
            --border-color: #dadce0;
            --code-bg: #f8f9fa;
            --highlight-bg: #e8f0fe;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Noto Sans', sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background-color: var(--bg-color);
            font-size: 16px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* Header Section */
        .header {
            text-align: center;
            margin-bottom: 40px;
        }

        .title {
            font-size: 2.2em;
            font-weight: 700;
            color: var(--text-color);
            margin-bottom: 8px;
            line-height: 1.3;
        }

        .subtitle {
            font-size: 1.1em;
            color: var(--secondary-color);
            font-weight: 400;
            margin-bottom: 25px;
        }

        /* Authors */
        .authors {
            margin-bottom: 15px;
            font-size: 1.05em;
        }

        .authors a {
            color: var(--primary-color);
            text-decoration: none;
        }

        .authors a:hover {
            text-decoration: underline;
        }

        .author-note {
            font-size: 0.85em;
            color: var(--secondary-color);
        }

        .affiliations {
            color: var(--secondary-color);
            font-size: 0.95em;
            margin-bottom: 20px;
        }

        .affiliation-logo {
            height: 28px;
            vertical-align: middle;
            margin-left: 8px;
        }

        /* Links */
        .links {
            display: flex;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
            margin-bottom: 30px;
        }

        .link-btn {
            display: inline-block;
            padding: 8px 16px;
            background-color: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 6px;
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
            font-size: 1em;
            transition: all 0.2s ease;
        }

        .link-btn:hover {
            background-color: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        .author-note {
            font-size: 0.85em;
            color: var(--secondary-color);
            margin-bottom: 20px;
        }

        /* Sections */
        section {
            margin-bottom: 45px;
        }

        h2 {
            font-size: 1.5em;
            font-weight: 600;
            color: var(--text-color);
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--border-color);
        }

        h3 {
            font-size: 1.2em;
            font-weight: 600;
            color: var(--text-color);
            margin-bottom: 15px;
            margin-top: 25px;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        /* Abstract */
        .abstract {
            background-color: var(--code-bg);
            padding: 25px 30px;
            border-radius: 10px;
            border-left: 4px solid var(--accent-color);
        }

        .abstract p {
            margin-bottom: 0;
        }

        /* Figures */
        .figure {
            text-align: center;
            margin: 30px 0;
        }

        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .figure-caption {
            margin-top: 15px;
            font-size: 0.9em;
            color: var(--secondary-color);
            text-align: center;
            max-width: 85%;
            margin-left: auto;
            margin-right: auto;
        }

        .figure-caption strong {
            color: var(--text-color);
        }

        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 25px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9em;
        }

        th, td {
            padding: 12px 15px;
            text-align: center;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background-color: var(--code-bg);
            font-weight: 600;
            color: var(--text-color);
        }

        tr:hover {
            background-color: var(--code-bg);
        }

        .highlight-row {
            background-color: var(--highlight-bg) !important;
        }

        .best {
            font-weight: 700;
            color: var(--primary-color);
        }

        .second-best {
            text-decoration: underline;
        }

        /* Key Results */
        .key-results {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .result-card {
            background: linear-gradient(135deg, var(--highlight-bg) 0%, var(--code-bg) 100%);
            padding: 25px;
            border-radius: 12px;
            text-align: center;
            border: 1px solid var(--border-color);
        }

        .result-value {
            font-size: 2em;
            font-weight: 700;
            color: var(--primary-color);
            display: block;
        }

        .result-label {
            font-size: 0.9em;
            color: var(--secondary-color);
            margin-top: 5px;
        }

        /* BibTeX */
        .bibtex {
            background-color: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            font-family: 'Source Code Pro', monospace;
            font-size: 0.85em;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        /* Method Steps */
        .method-steps {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        .method-step {
            display: flex;
            gap: 20px;
            align-items: flex-start;
        }

        .step-number {
            background-color: var(--accent-color);
            color: white;
            width: 36px;
            height: 36px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            flex-shrink: 0;
        }

        .step-content h4 {
            font-weight: 600;
            margin-bottom: 5px;
            color: var(--text-color);
        }

        .step-content p {
            color: var(--secondary-color);
            margin-bottom: 0;
            text-align: left;
        }

        /* Comparison */
        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }

        .comparison-item {
            text-align: center;
        }

        .comparison-item img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .comparison-label {
            margin-top: 10px;
            font-weight: 500;
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 30px 0;
            border-top: 1px solid var(--border-color);
            color: var(--secondary-color);
            font-size: 0.9em;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .title {
                font-size: 1.6em;
            }

            .comparison-grid {
                grid-template-columns: 1fr;
            }

            .key-results {
                grid-template-columns: 1fr 1fr;
            }

            th, td {
                padding: 8px 10px;
                font-size: 0.85em;
            }
        }

        /* LLM Logos */
        .llm-logo {
            height: 1.2em;
            vertical-align: middle;
            margin-right: 4px;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header -->
        <header class="header">
            <h1 class="title">AlignUSER</h1>
            <p class="subtitle">Human-Aligned LLM Agents via World Models for Recommender System Evaluation</p>

            <div class="authors">
                <a href="index.html">Nicolas Bougie</a><sup>1*</sup>,
                <a href="#">Gian Marconi Marconi</a><sup>1</sup>,
                <a href="#">Tony Yip</a><sup>1</sup>,
                <a href="#">Narimasa Watanabe</a><sup>1</sup>
            </div>

            <div class="affiliations">
                <sup>1</sup>Woven by Toyota
            </div>

            <p class="author-note"><sup>*</sup>Corresponding author</p>

            <div class="links">
                <a href="https://arxiv.org/abs/2601.00930" class="link-btn" target="_blank">[arXiv]</a>
                <a href="#" class="link-btn">[Code]</a>
            </div>
        </header>

        <!-- Abstract -->
        <section>
            <h2>Abstract</h2>
            <div class="abstract">
                <p>
                    Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce <strong>AlignUSER</strong>, a framework that learns <em>world-model-driven</em> agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate <strong>closer alignment with genuine humans</strong> than prior work, both at the micro and macro levels.
                </p>
            </div>
        </section>

        <!-- Key Results -->
        <section>
            <h2>Key Results</h2>
            <div class="key-results">
                <div class="result-card">
                    <span class="result-value">52.9%</span>
                    <span class="result-label">Action Generation Accuracy<br>(vs. 24.2% prior best)</span>
                </div>
                <div class="result-card">
                    <span class="result-value">89.3%</span>
                    <span class="result-label">Thought-Action Consistency</span>
                </div>
                <div class="result-card">
                    <span class="result-value">0.43</span>
                    <span class="result-label">Rating RMSE<br>(vs. 0.50 SimUSER)</span>
                </div>
                <div class="result-card">
                    <span class="result-value">4.58</span>
                    <span class="result-label">Human-Likeness Score<br>(5-point scale)</span>
                </div>
            </div>
        </section>

        <!-- Overview -->
        <section>
            <h2>Overview</h2>
            <p>
                AlignUSER learns to simulate realistic user behavior by combining two key insights: (1) agents should understand how the world works through next-state prediction, and (2) agents should align with human decisions through counterfactual reasoning. The framework first pretrains on environment dynamics, then learns from comparing human actions with alternative trajectories.
            </p>
            <div class="figure">
                <img src="illustration.jpg" alt="AlignUSER Framework Overview">
                <p class="figure-caption">
                    <strong>Figure 1:</strong> The AlignUSER framework for evaluating a recommender system by implicitly modeling a world model and exploring alternative scenarios. Green arrows show the human trajectory; red dashed arrows show counterfactual alternatives explored by the world model.
                </p>
            </div>
        </section>

        <!-- Motivation -->
        <section>
            <h2>Motivation</h2>
            <p>
                Existing LLM-based user simulators rely on few-shot prompting, treating the agent policy as a black box without explicit understanding of how actions shape future states. This leads to shallow behavior that reflects the model's priors rather than genuine user patterns. AlignUSER addresses this through:
            </p>

            <div class="method-steps">
                <div class="method-step">
                    <div class="step-number">1</div>
                    <div class="step-content">
                        <h4>World Modeling</h4>
                        <p>Train the agent to predict next states from state-action pairs, helping it internalize environment dynamics (e.g., clicking a product leads to a detail page).</p>
                    </div>
                </div>
                <div class="method-step">
                    <div class="step-number">2</div>
                    <div class="step-content">
                        <h4>Counterfactual Reasoning</h4>
                        <p>Generate alternative trajectories and prompt the LLM to compare them with human actions, identifying why human choices are better aligned with persona and context.</p>
                    </div>
                </div>
                <div class="method-step">
                    <div class="step-number">3</div>
                    <div class="step-content">
                        <h4>Policy Learning</h4>
                        <p>Train the policy to jointly predict chain-of-thought reasoning and expert actions, creating an agent that understands both what to do and why.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Framework -->
        <section>
            <h2>Counterfactual Reflection</h2>
            <p>
                For each human transition, we sample alternative actions that the current policy considers plausible but differ from the demonstrated action. The agent then reasons about why the human choice is better in context, more aligned with persona, and leads to better future outcomes.
            </p>
            <div class="figure">
                <img src="reasoning.jpg" alt="Counterfactual Reflection Process">
                <p class="figure-caption">
                    <strong>Figure 2:</strong> Counterfactual reflection from counterfactual trajectories. The LLM agent compares human actions with alternatives, generating reasoning signals that explain (1) contextual fit, (2) persona alignment, and (3) long-term outcome improvement.
                </p>
            </div>
        </section>

        <!-- Experiments -->
        <section>
            <h2>Experiments</h2>

            <h3>Preference Alignment</h3>
            <p>
                We evaluate whether agents can identify items aligned with their human counterparts' tastes. AlignUSER significantly outperforms baselines across all datasets and distractor levels.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method (1:1)</th>
                            <th colspan="2">MovieLens</th>
                            <th colspan="2">AmazonBook</th>
                            <th colspan="2">Steam</th>
                        </tr>
                        <tr>
                            <th></th>
                            <th>Acc</th>
                            <th>F1</th>
                            <th>Acc</th>
                            <th>F1</th>
                            <th>Acc</th>
                            <th>F1</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>RecAgent</td>
                            <td>0.581</td>
                            <td>0.621</td>
                            <td>0.604</td>
                            <td>0.659</td>
                            <td>0.627</td>
                            <td>0.650</td>
                        </tr>
                        <tr>
                            <td>Agent4Rec</td>
                            <td>0.691</td>
                            <td>0.698</td>
                            <td>0.719</td>
                            <td>0.700</td>
                            <td>0.689</td>
                            <td>0.679</td>
                        </tr>
                        <tr>
                            <td>SimUSER</td>
                            <td>0.791</td>
                            <td>0.777</td>
                            <td>0.822</td>
                            <td>0.790</td>
                            <td>0.791</td>
                            <td>0.794</td>
                        </tr>
                        <tr class="highlight-row">
                            <td>AlignUSER</td>
                            <td class="second-best">0.820</td>
                            <td class="second-best">0.817</td>
                            <td class="second-best">0.843</td>
                            <td class="second-best">0.830</td>
                            <td class="second-best">0.814</td>
                            <td class="second-best">0.834</td>
                        </tr>
                        <tr class="highlight-row">
                            <td>AlignUSER+</td>
                            <td class="best">0.832</td>
                            <td class="best">0.827</td>
                            <td class="best">0.855</td>
                            <td class="best">0.842</td>
                            <td class="best">0.827</td>
                            <td class="best">0.846</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Rating Prediction</h3>
            <p>
                AlignUSER achieves substantially lower rating prediction error than all baselines, demonstrating better understanding of user preferences.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th colspan="2">MovieLens</th>
                            <th colspan="2">AmazonBook</th>
                            <th colspan="2">Steam</th>
                        </tr>
                        <tr>
                            <th></th>
                            <th>RMSE</th>
                            <th>MAE</th>
                            <th>RMSE</th>
                            <th>MAE</th>
                            <th>RMSE</th>
                            <th>MAE</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Agent4Rec</td>
                            <td>0.761</td>
                            <td>0.714</td>
                            <td>0.879</td>
                            <td>0.671</td>
                            <td>0.758</td>
                            <td>0.688</td>
                        </tr>
                        <tr>
                            <td>SimUSER</td>
                            <td>0.502</td>
                            <td>0.446</td>
                            <td>0.568</td>
                            <td>0.421</td>
                            <td>0.587</td>
                            <td>0.532</td>
                        </tr>
                        <tr class="highlight-row">
                            <td>AlignUSER</td>
                            <td class="second-best">0.469</td>
                            <td class="second-best">0.415</td>
                            <td class="second-best">0.513</td>
                            <td class="second-best">0.399</td>
                            <td class="second-best">0.534</td>
                            <td class="second-best">0.501</td>
                        </tr>
                        <tr class="highlight-row">
                            <td>AlignUSER+</td>
                            <td class="best">0.429</td>
                            <td class="best">0.387</td>
                            <td class="best">0.465</td>
                            <td class="best">0.374</td>
                            <td class="best">0.497</td>
                            <td class="best">0.483</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Action Alignment</h3>
            <p>
                On next-action prediction, AlignUSER dramatically outperforms both general-purpose LLMs and specialized user simulation methods.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Action Gen. (Acc)</th>
                            <th>Action Type (F1)</th>
                            <th>Click Type (F1)</th>
                            <th>Session Outcome (F1)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><img src="OpenAI_Logo.png" class="llm-logo">GPT-4.1</td>
                            <td>21.51</td>
                            <td>48.78</td>
                            <td>44.47</td>
                            <td>47.54</td>
                        </tr>
                        <tr>
                            <td><img src="claude.png" class="llm-logo">Claude-3.7</td>
                            <td>10.75</td>
                            <td>31.58</td>
                            <td>27.27</td>
                            <td>43.52</td>
                        </tr>
                        <tr>
                            <td><img src="meta-color.png" class="llm-logo">Llama-3.3</td>
                            <td>8.31</td>
                            <td>24.29</td>
                            <td>19.99</td>
                            <td>36.64</td>
                        </tr>
                        <tr>
                            <td>SimUSER</td>
                            <td>24.21</td>
                            <td>52.44</td>
                            <td>48.68</td>
                            <td>59.63</td>
                        </tr>
                        <tr class="highlight-row">
                            <td><img src="Qwen_logo.png" class="llm-logo">AlignUSER</td>
                            <td class="second-best">51.47</td>
                            <td class="second-best">69.81</td>
                            <td class="second-best">66.29</td>
                            <td class="second-best">78.07</td>
                        </tr>
                        <tr class="highlight-row">
                            <td><img src="Qwen_logo.png" class="llm-logo">AlignUSER+</td>
                            <td class="best">52.92</td>
                            <td class="best">71.94</td>
                            <td class="best">66.88</td>
                            <td class="best">80.52</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Thought and Persona Consistency</h3>
            <p>
                On the OPeRA dataset with human rationales, AlignUSER achieves 89.3% thought-action consistency and produces session statistics closest to real human behavior.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Thought-Action (%)</th>
                            <th>Persona-Behavior (%)</th>
                            <th>Pages/Session</th>
                            <th>Purchase Gap (%)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Human (OPeRA)</td>
                            <td>--</td>
                            <td>--</td>
                            <td>5.3</td>
                            <td>--</td>
                        </tr>
                        <tr>
                            <td>RecAgent</td>
                            <td>49.5</td>
                            <td>46.7</td>
                            <td>3.5</td>
                            <td>16.3</td>
                        </tr>
                        <tr>
                            <td>Agent4Rec</td>
                            <td>55.8</td>
                            <td>52.4</td>
                            <td>4.0</td>
                            <td>12.1</td>
                        </tr>
                        <tr>
                            <td>SimUSER</td>
                            <td>64.3</td>
                            <td>61.5</td>
                            <td>4.6</td>
                            <td>9.9</td>
                        </tr>
                        <tr class="highlight-row">
                            <td>AlignUSER</td>
                            <td class="second-best">86.7</td>
                            <td class="second-best">82.4</td>
                            <td class="best">5.1</td>
                            <td class="second-best">2.5</td>
                        </tr>
                        <tr class="highlight-row">
                            <td>AlignUSER+</td>
                            <td class="best">89.3</td>
                            <td class="best">85.6</td>
                            <td class="best">5.1</td>
                            <td class="best">2.1</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- Datasets -->
        <section>
            <h2>Datasets</h2>
            <p>We evaluate AlignUSER on four diverse datasets spanning different recommendation domains:</p>
            <ul style="margin-left: 20px; margin-top: 10px;">
                <li><strong>MovieLens-1M:</strong> ~1 million movie ratings from 6,040 users over 3,706 movies</li>
                <li><strong>Steam:</strong> User-game interactions with English-language reviews</li>
                <li><strong>AmazonBook:</strong> Book ratings and reviews from Amazon</li>
                <li><strong>OPeRA:</strong> Real-world shopping sessions with persona surveys, actions, and self-reported rationales</li>
            </ul>
        </section>

        <!-- BibTeX -->
        <section>
            <h2>BibTeX</h2>
            <div class="bibtex">@article{bougie2025alignuser,
  title={AlignUSER: Human-Aligned LLM Agents via World Models
         for Recommender System Evaluation},
  author={Bougie, Nicolas and Marconi, Gian Marconi and
          Yip, Tony and Watanabe, Narimasa},
  journal={arXiv preprint arXiv:2601.00930},
  year={2025}
}</div>
        </section>

        <footer>
            <p>AlignUSER - <a href="index.html" style="color: var(--primary-color); text-decoration: none;">Nicolas Bougie</a> - Woven by Toyota</p>
        </footer>
    </div>
</body>
</html>
